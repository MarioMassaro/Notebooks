{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUSDbxe2f55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efae26c1-572b-4e6f-a4be-7fa2bd992427"
      },
      "source": [
        "# descargamos la biblioteca de nltk\n",
        "!pip install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y846AiC3gMQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1875045f-e35c-4770-f19b-2e5640b9fbeb"
      },
      "source": [
        "# importamos nltk y las partes que nos interezan\n",
        "import nltk\n",
        "import random\n",
        "from nltk import classify, pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
        "\n",
        "# descargamos los paquetes de nltk para hacerlo mas simple\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ADlBknSgZbZ"
      },
      "source": [
        "# agregamos a una variable los archivos descargados\n",
        "positive = twitter_samples.strings('positive_tweets.json')\n",
        "negative = twitter_samples.strings('negative_tweets.json')\n",
        "stop_words = list(set(stopwords.words('english')))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8uOOti6gec7",
        "outputId": "9ad2351a-1708-4f9b-b017-c97afef117db"
      },
      "source": [
        "# con esto podemos ver el contenido de lo que descargamos\n",
        "print(positive[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9OKitCjgjbd"
      },
      "source": [
        "# ahora separaremos los contenidos de lo descargado para porder analizarlos por partes\n",
        "positive_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6CjT-MxgvYo",
        "outputId": "ce014b7f-e5e6-45bb-d6e2-f07cbe051ed9"
      },
      "source": [
        "# aqui podemos ver lo que hicimos en el apartado anterior\n",
        "print(positive_tokens[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['@Lamb2ja', 'Hey', 'James', '!', 'How', 'odd', ':/', 'Please', 'call', 'our', 'Contact', 'Centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'Many', 'thanks', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44UyCXpg7Dd"
      },
      "source": [
        "# def 1\n",
        "def clean(tokens):\n",
        "    tokens = [x for x in tokens if not x in stop_words]\n",
        "\n",
        "    l = WordNetLemmatizer()\n",
        "    lemmatized = []\n",
        "\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized.append(l.lemmatize(word, pos))\n",
        "    return lemmatized\n",
        "    \n",
        "# def 2\n",
        "\n",
        "positive_clean = []\n",
        "negative_clean = []\n",
        "\n",
        "for token in positive_tokens:\n",
        "    positive_clean.append(clean(token))\n",
        "\n",
        "for token in negative_tokens:\n",
        "    negative_clean.append(clean(token))\n",
        "\n",
        "    def final_token_generator(tokens):\n",
        "      for tokens in tokens:\n",
        "        yield dict([token, True] for token in tokens)\n",
        "\n",
        "# creamos los datasets positivos y negativos\n",
        "\n",
        "positive_model_tokens = final_token_generator(positive_clean)\n",
        "negative_model_tokens = final_token_generator(negative_clean)\n",
        "\n",
        "positive_dataset = [(token, \"Positive\") for token in positive_model_tokens]\n",
        "negative_dataset = [(token, \"Negative\") for token in negative_model_tokens]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJWOmMy7hcUE"
      },
      "source": [
        "# ahora creamos el dataset general que utilizaremos\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "# movemos un poco y separamos los positivos de los negativos\n",
        "random.shuffle(dataset)\n",
        "random.shuffle(dataset)\n",
        "random.shuffle(dataset)\n",
        "\n",
        "#separamos el dataset en train y test, sabien ya de antemano que usamos 14000 registros\n",
        "training = dataset[:7000]\n",
        "testing = dataset[7000:]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J53Mh8PDhhn9"
      },
      "source": [
        "# creamos el clasificador y entrenamos el modelo\n",
        "classifier = NaiveBayesClassifier.train(training)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr2ZhHW5hkkP",
        "outputId": "0c047e5a-c82e-4d41-b326-3f2df07b5deb"
      },
      "source": [
        "#aqui podemos ver la precision de nuestro modelo simple\n",
        "print(\"Accuracy:\", classify.accuracy(classifier, testing))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9946666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99BIlisHhmio"
      },
      "source": [
        "# creamos nuestra funcion para correr el programa\n",
        "def analyze(input):\n",
        "    custom_tokens = clean(word_tokenize(input))\n",
        "    return classifier.classify(dict([token, True] for token in custom_tokens))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uOOsBc6UhyTT",
        "outputId": "caabde90-386b-45dd-df52-409d2467efa8"
      },
      "source": [
        "# utilizamos el programa y probamos unas cuentas peticiones\n",
        "analyze(\"the days are turning darker and darker everyday\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "11nV56B38BGi",
        "outputId": "98d54c2f-e7bd-4e72-91b2-d727c05c5dcb"
      },
      "source": [
        "analyze(\"today i had a beautifull morning\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P5ny6gkO8GxA",
        "outputId": "7ddff9f9-2abd-46dd-a79a-1396a5c6f47b"
      },
      "source": [
        "analyze(\"I'm so tired of the same thing everyday\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YjdWeRzM-g9F",
        "outputId": "69d02f1c-5804-493b-b389-e5abf9d4a741"
      },
      "source": [
        "analyze(\"haha you are so funny\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Positive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}